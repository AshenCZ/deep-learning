{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "uppercase",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "8uxTNzOLoCSl",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 7
            },
            {
              "item_id": 8
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "222af09e-61e0-48f0-cade-0fa7cc8bae61",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1521580436680,
          "user_tz": -60,
          "elapsed": 11678,
          "user": {
            "displayName": "Tomáš Iser",
            "photoUrl": "//lh4.googleusercontent.com/-p8FdwnQWrMs/AAAAAAAAAAI/AAAAAAAAA0Q/Us7JQZEeMzU/s50-c-k-no/photo.jpg",
            "userId": "107716882151809473185"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/mixuala/colab_utils.git\n",
        "\n",
        "import os\n",
        "import colab_utils.tboard\n",
        "\n",
        "# set paths\n",
        "ROOT = %pwd\n",
        "LOG_DIR = os.path.join(ROOT, 'logs')\n",
        "print(ROOT, LOG_DIR)\n",
        "\n",
        "# will install `ngrok`, if necessary\n",
        "# will create `log_dir` if path does not exist\n",
        "colab_utils.tboard.launch_tensorboard( bin_dir=ROOT, log_dir=LOG_DIR )\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'colab_utils'...\n",
            "remote: Counting objects: 216, done.\u001b[K\n",
            "remote: Compressing objects: 100% (33/33), done.\u001b[K\n",
            "remote: Total 216 (delta 15), reused 40 (delta 12), pack-reused 171\u001b[K\n",
            "Receiving objects: 100% (216/216), 60.19 KiB | 5.02 MiB/s, done.\n",
            "Resolving deltas: 100% (83/83), done.\n",
            "/content /content/logs\n",
            "calling wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip ...\n",
            "calling unzip ngrok-stable-linux-amd64.zip ...\n",
            "ngrok installed. path=/content/ngrok\n",
            "status: tensorboard=False, ngrok=False\n",
            "tensorboard url= http://c530266d.ngrok.io\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'http://c530266d.ngrok.io'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "metadata": {
        "id": "JB-yGYFkrH5R",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 12
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "cellView": "code",
        "outputId": "8048466a-35d8-4b56-c07a-89faf9103ce8",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1521580826812,
          "user_tz": -60,
          "elapsed": 378682,
          "user": {
            "displayName": "Tomáš Iser",
            "photoUrl": "//lh4.googleusercontent.com/-p8FdwnQWrMs/AAAAAAAAAAI/AAAAAAAAA0Q/Us7JQZEeMzU/s50-c-k-no/photo.jpg",
            "userId": "107716882151809473185"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# Loads an uppercase dataset.\n",
        "# - The dataset either uses a specified alphabet, or constructs an alphabet of\n",
        "#   specified size consisting of most frequent characters.\n",
        "# - The batches are generated using a sliding window of given size,\n",
        "#   i.e., for a character, we generate left `window` characters, the character\n",
        "#   itself and right `window` characters, 2 * `window` +1 in total.\n",
        "# - The batches can be either generated using `next_batch`+`epoch_finished`,\n",
        "#   or all data in the original order can be generated using `all_data`.\n",
        "class Dataset:\n",
        "    def __init__(self, filename, window, alphabet):\n",
        "        self._window = window\n",
        "\n",
        "        # Load the data\n",
        "        with open(filename, \"r\", encoding=\"utf-8\") as file:\n",
        "            self._text = file.read()\n",
        "\n",
        "        # Create alphabet_map\n",
        "        alphabet_map = {\"<pad>\": 0, \"<unk>\": 1}\n",
        "        if not isinstance(alphabet, int):\n",
        "            for index, letter in enumerate(alphabet):\n",
        "                alphabet_map[letter] = index\n",
        "        else:\n",
        "            # Find most frequent characters\n",
        "            freqs = {}\n",
        "            for char in self._text:\n",
        "                char = char.lower()\n",
        "                freqs[char] = freqs.get(char, 0) + 1\n",
        "\n",
        "            most_frequent = sorted(freqs.items(), key=lambda item:item[1], reverse=True)\n",
        "            for i, (char, freq) in enumerate(most_frequent, len(alphabet_map)):\n",
        "                alphabet_map[char] = i\n",
        "                if len(alphabet_map) >= alphabet: break\n",
        "\n",
        "        # Remap input characters using the alphabet_map\n",
        "        self._lcletters = np.zeros(len(self._text) + 2 * window, np.uint8)\n",
        "        self._labels = np.zeros(len(self._text), np.bool)\n",
        "        for i in range(len(self._text)):\n",
        "            char = self._text[i].lower()\n",
        "            if char not in alphabet_map: char = \"<unk>\"\n",
        "            self._lcletters[i + window] = alphabet_map[char]\n",
        "            self._labels[i] = self._text[i].isupper()\n",
        "\n",
        "        # Compute alphabet\n",
        "        self._alphabet = [\"\"] * len(alphabet_map)\n",
        "        for key, value in alphabet_map.items():\n",
        "            self._alphabet[value] = key\n",
        "\n",
        "        self._permutation = np.random.permutation(len(self._text))\n",
        "\n",
        "    def _create_batch(self, permutation):\n",
        "        batch_windows = np.zeros([len(permutation), 2 * self._window + 1], np.int32)\n",
        "        for i in range(0, 2 * self._window + 1):\n",
        "            batch_windows[:, i] = self._lcletters[permutation + i]\n",
        "        return batch_windows, self._labels[permutation]\n",
        "\n",
        "    def print_results(self, batch_size, network, dataset):\n",
        "      utf8stdout = open(\"output_{}.txt\".format(dataset), 'w', encoding='utf-8', closefd=True)\n",
        "      iSum = 0\n",
        "      #while not self.epoch_finished():\n",
        "          #windows, labels = self.next_batch(batch_size)\n",
        "      windows, labels = self.all_data()\n",
        "      predictions, _ = network.evaluate(dataset, windows, labels)\n",
        "\n",
        "      for i in range(len(predictions)):\n",
        "          character = self.text[iSum].lower()\n",
        "          if (predictions[i] == True):\n",
        "              character = character.upper()\n",
        "          print(character, end='', file=utf8stdout)\n",
        "          iSum += 1\n",
        "\n",
        "      \n",
        "    @property\n",
        "    def alphabet(self):\n",
        "        return self._alphabet\n",
        "\n",
        "    @property\n",
        "    def text(self):\n",
        "        return self._text\n",
        "\n",
        "    @property\n",
        "    def labels(self):\n",
        "        return self._labels\n",
        "\n",
        "    def all_data(self):\n",
        "        return self._create_batch(np.arange(len(self._text)))\n",
        "\n",
        "    def next_batch(self, batch_size):\n",
        "        batch_size = min(batch_size, len(self._permutation))\n",
        "        batch_perm, self._permutation = self._permutation[:batch_size], self._permutation[batch_size:]\n",
        "        return self._create_batch(batch_perm)\n",
        "\n",
        "    def epoch_finished(self):\n",
        "        if len(self._permutation) == 0:\n",
        "            self._permutation = np.random.permutation(len(self._text))\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "\n",
        "class Network:\n",
        "    def __init__(self, seed=42):\n",
        "        # Create an empty graph and a session\n",
        "        graph = tf.Graph()\n",
        "        graph.seed = seed\n",
        "        self.session = tf.Session(graph = graph, config=tf.ConfigProto())\n",
        "\n",
        "    def construct(self, args):\n",
        "        with self.session.graph.as_default():\n",
        "            # Inputs\n",
        "            self.windows = tf.placeholder(tf.int32, [None, 2 * args.window + 1], name=\"windows\")\n",
        "            self.labels = tf.placeholder(tf.int32, [None], name=\"labels\") # Or you can use tf.int32\n",
        "            self.trainingMode = tf.placeholder(tf.bool)\n",
        "\n",
        "            hot_repre = tf.one_hot(self.windows, args.alphabet_size)\n",
        "\n",
        "            # Architecture\n",
        "            flattened_images = tf.layers.flatten(hot_repre, name=\"flatten\")\n",
        "\n",
        "            hidden_size = math.ceil(7.5 * (args.window * 2 + 1) * args.alphabet_size)\n",
        "            print(\"Hidden size: \", hidden_size)\n",
        "\n",
        "            hidden_layer = tf.layers.dense(flattened_images, hidden_size, activation=tf.nn.relu, name=\"Relu_1\")\n",
        "            hidden_layer_dropout = tf.layers.dropout(hidden_layer, rate=0.5, training=self.trainingMode, name=\"dropout_layer\")\n",
        "            hidden_2 = tf.layers.dense(hidden_layer_dropout, math.ceil(hidden_size / 30), activation=tf.nn.relu, name=\"Relu_2\")\n",
        "            output_layer = tf.layers.dense(hidden_2, 2, activation=None, name=\"output_layer\")\n",
        "            self.predictions = tf.cast(tf.argmax(output_layer, axis=1, output_type=tf.int32), tf.bool)\n",
        "\n",
        "            # Training\n",
        "            loss = tf.losses.sparse_softmax_cross_entropy(self.labels, output_layer, scope=\"loss\")\n",
        "            global_step = tf.train.create_global_step()\n",
        "            self.training = tf.train.AdamOptimizer(1e-3).minimize(loss, global_step=global_step, name=\"training\")\n",
        "\n",
        "            # Summaries\n",
        "            self.accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.cast(self.labels, tf.bool), self.predictions), tf.float32))\n",
        "            summary_writer = tf.contrib.summary.create_file_writer(args.logdir, flush_millis=10 * 1000)\n",
        "            self.summaries = {}\n",
        "            with summary_writer.as_default(), tf.contrib.summary.record_summaries_every_n_global_steps(100):\n",
        "                self.summaries[\"train\"] = [tf.contrib.summary.scalar(\"train/loss\", loss),\n",
        "                                           tf.contrib.summary.scalar(\"train/accuracy\", self.accuracy)]\n",
        "            with summary_writer.as_default(), tf.contrib.summary.always_record_summaries():\n",
        "                for dataset in [\"dev\", \"test\"]:\n",
        "                    self.summaries[dataset] = [tf.contrib.summary.scalar(dataset + \"/loss\", loss),\n",
        "                                               tf.contrib.summary.scalar(dataset + \"/accuracy\", self.accuracy)]\n",
        "\n",
        "            # Initialize variables\n",
        "            self.session.run(tf.global_variables_initializer())\n",
        "            with summary_writer.as_default():\n",
        "                tf.contrib.summary.initialize(session=self.session, graph=self.session.graph)\n",
        "\n",
        "    def train(self, windows, labels):\n",
        "        self.session.run([self.training, self.summaries[\"train\"]], {self.windows: windows, self.labels: labels, self.trainingMode: True})\n",
        "\n",
        "    def evaluate(self, dataset, windows, labels):\n",
        "        return self.session.run([self.predictions, self.summaries[dataset]], {self.windows: windows, self.labels: labels, self.trainingMode: False})\n",
        "\n",
        "class Args(object):\n",
        "  pass\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import argparse\n",
        "    import datetime\n",
        "    import os\n",
        "    import re\n",
        "\n",
        "    # Fix random seed\n",
        "    np.random.seed(42)\n",
        "\n",
        "    # Parse arguments\n",
        "    # parser = argparse.ArgumentParser()\n",
        "    # parser.add_argument(\"--alphabet_size\", default=60, type=int, help=\"Alphabet size.\")\n",
        "    # parser.add_argument(\"--batch_size\", default=1000, type=int, help=\"Batch size.\")\n",
        "    # parser.add_argument(\"--epochs\", default=1, type=int, help=\"Number of epochs.\")\n",
        "    # parser.add_argument(\"--threads\", default=1, type=int, help=\"Maximum number of threads to use.\")\n",
        "    # parser.add_argument(\"--window\", default=3, type=int, help=\"Size of the window to use.\")\n",
        "    # args = parser.parse_args()\n",
        "    \n",
        "    args = Args()\n",
        "    args.alphabet_size = 60\n",
        "    args.batch_size = 1000\n",
        "    args.epochs = 5\n",
        "    args.window = 4\n",
        "\n",
        "    # Create logdir name\n",
        "    args.logdir = \"logs/{}-{}-{}\".format(\n",
        "        \"./\",\n",
        "        datetime.datetime.now().strftime(\"%Y-%m-%d_%H%M%S\"),\n",
        "        \",\".join((\"{}={}\".format(re.sub(\"(.)[^_]*_?\", r\"\\1\", key), value) for key, value in sorted(vars(args).items())))\n",
        "    )\n",
        "    if not os.path.exists(\"logs\"): os.mkdir(\"logs\") # TF 1.6 will do this by itself\n",
        "\n",
        "    # Load the data\n",
        "    train = Dataset(\"uppercase_data_train.txt\", args.window, alphabet=args.alphabet_size)\n",
        "    dev = Dataset(\"uppercase_data_dev.txt\", args.window, alphabet=train.alphabet)\n",
        "\n",
        "    # For ReCodex\n",
        "    test = Dataset(\"uppercase_data_test.txt\", args.window, alphabet=train.alphabet)\n",
        "\n",
        "    # Construct the network\n",
        "    network = Network()\n",
        "    network.construct(args)\n",
        "\n",
        "    # Train\n",
        "    for i in range(args.epochs):\n",
        "        print(\"Starting epoch \", i)\n",
        "        while not train.epoch_finished():\n",
        "            windows, labels = train.next_batch(args.batch_size)\n",
        "            network.train(windows, labels)\n",
        "\n",
        "        print(\"Training finished at \", i)\n",
        "        dev_windows, dev_labels = dev.all_data()\n",
        "        res = network.evaluate(\"dev\", dev_windows, dev_labels)\n",
        "        print(\"Epoch \", i, \", Dev acc: \", res[1])\n",
        "\n",
        "    \n",
        "    dev.print_results(args.batch_size, network, \"dev\")\n",
        "    test.print_results(args.batch_size, network, \"test\")\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hidden size:  4050\n",
            "Starting epoch  0\n",
            "Training finished at  0\n",
            "Epoch  0 , Dev acc:  [True, True]\n",
            "Starting epoch  1\n",
            "Training finished at  1\n",
            "Epoch  1 , Dev acc:  [True, True]\n",
            "Starting epoch  2\n",
            "Training finished at  2\n",
            "Epoch  2 , Dev acc:  [True, True]\n",
            "Starting epoch  3\n",
            "Training finished at  3\n",
            "Epoch  3 , Dev acc:  [True, True]\n",
            "Starting epoch  4\n",
            "Training finished at  4\n",
            "Epoch  4 , Dev acc:  [True, True]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NAZA3sp8raA1",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 1
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e1bf458f-29fc-4ed7-d94f-e879bfde3797",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1521637302696,
          "user_tz": -60,
          "elapsed": 1006,
          "user": {
            "displayName": "Štěpán Hojdar",
            "photoUrl": "//lh4.googleusercontent.com/-5rDXA_8cvxY/AAAAAAAAAAI/AAAAAAAAAJE/98aexWZzxoA/s50-c-k-no/photo.jpg",
            "userId": "102759379961150890704"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "datalab\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dZaSLjp8rmLS",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.download('output_dev.txt')\n",
        "files.download('output_test.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "n7V8c6NByELr",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# memory footprint support libraries/code\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "GPUs = GPU.getGPUs()\n",
        "# XXX: only one GPU on Colab and isn’t guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        " process = psutil.Process(os.getpid())\n",
        " print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" I Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        " print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "printm()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sbIL3uDkfzTi",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 1
            },
            {
              "item_id": 3
            }
          ],
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "6572c38e-c28c-4b21-e51c-e2beb0996824",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1521580354633,
          "user_tz": -60,
          "elapsed": 56046,
          "user": {
            "displayName": "Tomáš Iser",
            "photoUrl": "//lh4.googleusercontent.com/-p8FdwnQWrMs/AAAAAAAAAAI/AAAAAAAAA0Q/Us7JQZEeMzU/s50-c-k-no/photo.jpg",
            "userId": "107716882151809473185"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-80cd93bc-d717-4bc9-8cfc-8726c214546a\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-80cd93bc-d717-4bc9-8cfc-8726c214546a\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving uppercase_data_train.txt to uppercase_data_train.txt\n",
            "User uploaded file \"uppercase_data_train.txt\" with length 6536807 bytes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DgzHRfnryvJH",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!kill -9 -1\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}