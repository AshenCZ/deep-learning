{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "template",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "-Q920qN8sa-3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Tensorboard and data setup"
      ]
    },
    {
      "metadata": {
        "id": "cHKsM-D5soBD",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Data, clone git\n",
        "!git clone https://ashencz:@github.com/AshenCZ/deep-learning.git\n",
        "\n",
        "# Change dir to the project\n",
        "# TODO: Change directory of project\n",
        "%cd ./deep-learning/mnist_comp\n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-JlYBtcDsljH",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Tensorboard\n",
        "\n",
        "!git clone https://github.com/mixuala/colab_utils.git\n",
        "\n",
        "import os\n",
        "import colab_utils.tboard\n",
        "\n",
        "# set paths\n",
        "ROOT = %pwd\n",
        "LOG_DIR = os.path.join(ROOT, 'logs')\n",
        "print(ROOT, LOG_DIR)\n",
        "\n",
        "# will install `ngrok`, if necessary\n",
        "# will create `log_dir` if path does not exist\n",
        "colab_utils.tboard.launch_tensorboard( bin_dir=ROOT, log_dir=LOG_DIR )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nVxCJM3uftYz",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Find out how much GPU memory you have left\n",
        "\n",
        "# memory footprint support libraries/code\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "GPUs = GPU.getGPUs()\n",
        "# XXX: only one GPU on Colab and isnâ€™t guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        " process = psutil.Process(os.getpid())\n",
        " print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" I Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        " print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "printm()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "86w6IklEs15Q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Code"
      ]
    },
    {
      "metadata": {
        "id": "8eACbaGZLqxX",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "class Args(object):\n",
        "  pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EGDSJT2UuUhb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Network"
      ]
    },
    {
      "metadata": {
        "id": "Wr_KRGVrtERQ",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Network\n",
        "\n",
        "class Network:\n",
        "    WIDTH = 28\n",
        "    HEIGHT = 28\n",
        "    LABELS = 10\n",
        "\n",
        "    def __init__(self, threads, seed=42):\n",
        "        # Create an empty graph and a session\n",
        "        graph = tf.Graph()\n",
        "        graph.seed = seed\n",
        "        self.session = tf.Session(graph = graph, config=tf.ConfigProto(inter_op_parallelism_threads=threads,\n",
        "                                                                       intra_op_parallelism_threads=threads))\n",
        "\n",
        "    def construct(self, args):\n",
        "        with self.session.graph.as_default():\n",
        "            # Inputs\n",
        "            self.images = tf.placeholder(tf.float32, [None, self.WIDTH, self.HEIGHT, 1], name=\"images\")\n",
        "            self.labels = tf.placeholder(tf.int64, [None], name=\"labels\")\n",
        "            self.is_training = tf.placeholder(tf.bool, [], name=\"is_training\")\n",
        "\n",
        "            previous_layer = self.images\n",
        "\n",
        "            layers = args.cnn.split(\",\")\n",
        "            for l in layers:\n",
        "                params = l.split(\"-\")\n",
        "                if l[0] == 'C':\n",
        "                    filters = int(params[1])\n",
        "                    kernel_size = int(params[2])\n",
        "                    stride = int(params[3])\n",
        "                    padding_param = params[4]\n",
        "                    # print(\"Convo\"+str(filters)+str(kernel_size)+str(stride)+str(padding_param))\n",
        "                    next_layer = tf.layers.conv2d(previous_layer, filters, kernel_size, stride, padding=padding_param, activation=tf.nn.relu, name=\"Convo\"+str(filters)+str(kernel_size)+str(stride)+str(padding_param))\n",
        "                elif l[0] == 'M':\n",
        "                    kernel_size = int(params[1])\n",
        "                    stride = int(params[2])\n",
        "                    # print(\"MaxPool\"+str(kernel_size)+str(stride))\n",
        "                    next_layer = tf.layers.max_pooling2d(previous_layer, kernel_size, stride, name=\"MaxPool\"+str(kernel_size)+str(stride))\n",
        "                elif l[0] == 'F':\n",
        "                    next_layer = tf.layers.flatten(previous_layer, name=\"Flatten\")\n",
        "                    # print(\"Flatten\")\n",
        "                elif l[0] == 'R':\n",
        "                    size = int(params[1])\n",
        "                    next_layer = tf.layers.dense(previous_layer, size, activation=tf.nn.relu, name=\"dense\"+str(size))\n",
        "                    # print(\"dense\"+str(size))\n",
        "                elif l[0] == 'D':\n",
        "                    # rate = float(params[1])\n",
        "                    next_layer = tf.layers.dropout(inputs=previous_layer, rate=0.4, training=self.is_training)\n",
        "\n",
        "                else:\n",
        "                    raise Exception(\"What.\")\n",
        "                previous_layer = next_layer\n",
        "\n",
        "            output_layer = tf.layers.dense(previous_layer, self.LABELS, activation=None, name=\"output_layer\")\n",
        "            self.predictions = tf.argmax(output_layer, axis=1)\n",
        "\n",
        "            # Training\n",
        "            loss = tf.losses.sparse_softmax_cross_entropy(self.labels, output_layer, scope=\"loss\")\n",
        "            global_step = tf.train.create_global_step()\n",
        "            self.training = tf.train.AdamOptimizer().minimize(loss, global_step=global_step, name=\"training\")\n",
        "\n",
        "            # Summaries\n",
        "            self.accuracy = tf.reduce_mean(tf.cast(tf.equal(self.labels, self.predictions), tf.float32))\n",
        "            summary_writer = tf.contrib.summary.create_file_writer(args.logdir, flush_millis=10 * 1000)\n",
        "            self.summaries = {}\n",
        "            with summary_writer.as_default(), tf.contrib.summary.record_summaries_every_n_global_steps(100):\n",
        "                self.summaries[\"train\"] = [tf.contrib.summary.scalar(\"train/loss\", loss),\n",
        "                                           tf.contrib.summary.scalar(\"train/accuracy\", self.accuracy)]\n",
        "            with summary_writer.as_default(), tf.contrib.summary.always_record_summaries():\n",
        "                for dataset in [\"dev\", \"test\"]:\n",
        "                    self.summaries[dataset] = [tf.contrib.summary.scalar(dataset + \"/loss\", loss),\n",
        "                                               tf.contrib.summary.scalar(dataset + \"/accuracy\", self.accuracy)]\n",
        "\n",
        "            # Initialize variables\n",
        "            self.session.run(tf.global_variables_initializer())\n",
        "            with summary_writer.as_default():\n",
        "                tf.contrib.summary.initialize(session=self.session, graph=self.session.graph)\n",
        "\n",
        "    def train(self, images, labels):\n",
        "        self.session.run([self.training, self.summaries[\"train\"]], {self.images: images, self.labels: labels, self.is_training: True})\n",
        "\n",
        "    def evaluate(self, dataset, images, labels):\n",
        "        return self.session.run([self.predictions, self.accuracy, self.summaries[dataset]], {self.images: images, self.labels: labels, self.is_training: False})\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Q9B8aQP7uW8Y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Training"
      ]
    },
    {
      "metadata": {
        "id": "xw2KCMfUf4IM",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    import argparse\n",
        "    import datetime\n",
        "    import os\n",
        "    import re\n",
        "\n",
        "    # Fix random seed\n",
        "    np.random.seed(42)\n",
        "\n",
        "    # TODO: Arguments\n",
        "    args = Args()\n",
        "    args.epochs = 1\n",
        "    args.threads = 4\n",
        "    \n",
        "    args.batch_size = 50\n",
        "    args.cnn = \"C-32-5-1-same,M-2-2,C-64-5-1-same,M-2-2,F,R-1024,D,R-10\"    \n",
        "\n",
        "    # Create logdir name\n",
        "    args.logdir = \"logs/{}-{}-{}\".format(\n",
        "        \"./\",\n",
        "        datetime.datetime.now().strftime(\"%Y-%m-%d_%H%M%S\"),\n",
        "        \",\".join((\"{}={}\".format(re.sub(\"(.)[^_]*_?\", r\"\\1\", key), value) for key, value in sorted(vars(args).items())))\n",
        "    )\n",
        "    if not os.path.exists(\"logs\"): os.mkdir(\"logs\") # TF 1.6 will do this by itself\n",
        "    \n",
        "    # Load the data\n",
        "    from tensorflow.examples.tutorials import mnist\n",
        "    mnist = mnist.input_data.read_data_sets(\"data\", reshape=False, seed=42)\n",
        "\n",
        "    # Construct the network\n",
        "    network = Network(threads=args.threads)\n",
        "    network.construct(args)\n",
        "\n",
        "    # Train\n",
        "    for i in range(args.epochs):\n",
        "        while mnist.train.epochs_completed == i:\n",
        "            images, labels = mnist.train.next_batch(args.batch_size)\n",
        "            network.train(images, labels)\n",
        "\n",
        "        _, acc, _  = network.evaluate(\"dev\", mnist.validation.images, mnist.validation.labels)\n",
        "        print(i,\" Dev: {:.2f}\".format(100 * acc))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "e0bQ_ABP1_Ks",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Test set evaluation"
      ]
    },
    {
      "metadata": {
        "id": "oCJTmbya1qeJ",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Generate the predictions\n",
        "# TODO: Change predictions\n",
        "predictions, _, _ = network.evaluate(\"test\", mnist.test.images, mnist.test.labels)\n",
        "\n",
        "# Open the file\n",
        "time = datetime.datetime.now().time()\n",
        "last_file_name = \"output_test_\"+str(time.hour)+str(time.minute)+\".txt\"\n",
        "utf8stdout = open(last_file_name, 'w', encoding='utf-8', closefd=True)\n",
        "\n",
        "# Print the results into the file\n",
        "# TODO: Change format of output\n",
        "for label in predictions:\n",
        "  print(label, file=utf8stdout)\n",
        "\n",
        "# Finished\n",
        "print(\"File finished: \", last_file_name)\n",
        "utf8stdout.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "odIRXYbl592M",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Download the ouput data\n",
        "from google.colab import files\n",
        "files.download(last_file_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sBgBJ6wPGo_c",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## The rest"
      ]
    },
    {
      "metadata": {
        "id": "nZn6H5JW8y7Q",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Blank for tests\n",
        "!ls\n",
        "# !kill -9 -1"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}